{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DS 340W Modified Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get Environment for API Secret**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connect to Guardian API for News Articles about Specific Companies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Apple articles to data\\guardian_apple_articles.csv\n",
      "Saved Tesla articles to data\\guardian_tesla_articles.csv\n",
      "Saved Amazon articles to data\\guardian_amazon_articles.csv\n"
     ]
    }
   ],
   "source": [
    "def get_guardian_articles(query, start_date, end_date, api_key, max_pages=100):\n",
    "    url = \"https://content.guardianapis.com/search\"\n",
    "    news_data = []\n",
    "    page_size = 50 \n",
    "    \n",
    "    page = 1\n",
    "    while page <= max_pages:\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'from-date': start_date,\n",
    "            'to-date': end_date,\n",
    "            'api-key': api_key,\n",
    "            'page': page,\n",
    "            'page-size': page_size\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        articles = data['response']['results']\n",
    "        \n",
    "        for article in articles:\n",
    "            news_data.append({\n",
    "                'title': article['webTitle'],\n",
    "                'url': article['webUrl'],\n",
    "                'publishedAt': article['webPublicationDate']\n",
    "            })\n",
    "        \n",
    "        ## PAGINATION\n",
    "        if not data['response']['pages'] > page:\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        sleep(0.5) # RATE LIMITS \n",
    "    \n",
    "    return pd.DataFrame(news_data)\n",
    "\n",
    "start_date = '2019-12-31' # START DATE\n",
    "end_date = '2020-12-31' # END DATE (2020 FISCAL YEAR)\n",
    "queries = ['Apple', 'Tesla', 'Amazon']\n",
    "\n",
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True) # MAKE DATA DIRECTORY\n",
    "\n",
    "for query in queries:\n",
    "    year_start = datetime.strptime(start_date, \"%Y-%m-%d\") # PROPER QUERY FORMAT FOR DATES\n",
    "    year_end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    all_articles = pd.DataFrame()\n",
    "\n",
    "    while year_start < year_end:\n",
    "        next_year_end = min(year_start + timedelta(days=365), year_end) # 365 DAYS + START DATE\n",
    "        articles_df = get_guardian_articles(query, year_start.strftime(\"%Y-%m-%d\"), \n",
    "                                            next_year_end.strftime(\"%Y-%m-%d\"), \n",
    "                                            api_key, max_pages=100)\n",
    "        all_articles = pd.concat([all_articles, articles_df], ignore_index=True)\n",
    "        year_start = next_year_end + timedelta(days=1)\n",
    "\n",
    "    # SAVE FILES TO CSVS\n",
    "    file_name = f\"guardian_{query.lower()}_articles.csv\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    all_articles.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Saved {query} articles to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Download 2019-2020 Stock Data for Tesla, Apple, and Amazon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_fill_stock_data(symbol, start_date, end_date):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date) # YAHOO FINANCE STOCK DATA\n",
    "\n",
    "    all_dates = pd.date_range(start=start_date, end=end_date, freq='B')  # ONLY BUSINESS DAYS (WEEKDAYS)\n",
    "    stock_data = stock_data.reindex(all_dates)\n",
    "\n",
    "    stock_data['Close'] = stock_data['Close'].fillna(method='ffill') # FORWARD FILL (FILL ANY MISSING DATES W PREVIOUS DATE)\n",
    " \n",
    "    stock_data['Close'] = stock_data['Close'].fillna(method='bfill') # EXCEPTION HANDLER IN CASE FFILL DOESN'T WORK\n",
    "\n",
    "    return stock_data\n",
    "\n",
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True) # DATA FOLDER\n",
    "\n",
    "symbols = [\"AAPL\", \"AMZN\", \"TSLA\"] # STOCKS ANALYZED\n",
    "start_date = \"2019-12-31\"\n",
    "end_date = \"2020-12-31\"\n",
    "\n",
    "for symbol in symbols: # DOWNLOADER FOR EACH STOCK\n",
    "    stock_data = download_and_fill_stock_data(symbol, start_date, end_date)\n",
    "    \n",
    "    file_name = f\"{symbol.lower()}_stock_data_2019_2020.csv\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    stock_data.to_csv(file_path) # SAVE TO CSV\n",
    "    \n",
    "    print(f\"Saved {symbol} stock data to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment') # PRE TRAINED BERT MODEL FOR SENTIMENT ANALYSIS\n",
    "model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "stock_data_apple = pd.read_csv('data/aapl_stock_data_2019_2020.csv', index_col='Date', parse_dates=True) # READ HISTORICAL STOCK DATA\n",
    "stock_data_amazon = pd.read_csv('data/amzn_stock_data_2019_2020.csv', index_col='Date', parse_dates=True)\n",
    "stock_data_tesla = pd.read_csv('data/tsla_stock_data_2019_2020.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "stock_data_apple = stock_data_apple.sort_index() # SORT BY DATE (INDEX COLUMN)\n",
    "stock_data_amazon = stock_data_amazon.sort_index()\n",
    "stock_data_tesla = stock_data_tesla.sort_index()\n",
    "\n",
    "# ARTICLES FOR ALL COMPANIES\n",
    "df_apple = pd.read_csv(\"data/guardian_apple_articles.csv\")\n",
    "df_amazon = pd.read_csv(\"data/guardian_amazon_articles.csv\")\n",
    "df_tesla = pd.read_csv(\"data/guardian_tesla_articles.csv\")\n",
    "\n",
    "titles_apple = df_apple['title'].tolist()\n",
    "dates_apple = df_apple['publishedAt'].tolist()\n",
    "\n",
    "titles_amazon = df_amazon['title'].tolist()\n",
    "dates_amazon = df_amazon['publishedAt'].tolist()\n",
    "\n",
    "titles_tesla = df_tesla['title'].tolist()\n",
    "dates_tesla = df_tesla['publishedAt'].tolist()\n",
    "\n",
    "for stock_data in [stock_data_apple, stock_data_amazon, stock_data_tesla]:\n",
    "    if stock_data.index.tz is not None: # ACCOUNT FOR TIME ZONE NAIVE/AWARE (FOR NO ERRORS)\n",
    "        stock_data.index = stock_data.index.tz_convert(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price(symbol, date): # STOCK PRICE ON GIVEN DAY\n",
    "    date = pd.to_datetime(date)\n",
    "    \n",
    "    if date.tz is not None:\n",
    "        date = date.tz_convert(None)\n",
    "    \n",
    "    if symbol == \"AAPL\":\n",
    "        stock_data = stock_data_apple\n",
    "    elif symbol == \"AMZN\":\n",
    "        stock_data = stock_data_amazon\n",
    "    elif symbol == \"TSLA\":\n",
    "        stock_data = stock_data_tesla\n",
    "    \n",
    "    if date in stock_data.index:\n",
    "        return stock_data.loc[date, 'Close']\n",
    "    else:\n",
    "        previous_date = stock_data.index[stock_data.index.searchsorted(date) - 1] # USE PREVIOUS DATE IF DATA ISN'T AVAILABLE FOR CURRENT DATE\n",
    "        return stock_data.loc[previous_date, 'Close']\n",
    "\n",
    "def get_price_change(symbol, date): # GET PRICE DIFFERENCE FROM CURRENT DAY - PREVIOUS DAY\n",
    "    today_price = get_stock_price(symbol, date)\n",
    "    if today_price is None:\n",
    "        return None\n",
    "    next_day = pd.to_datetime(date) + pd.DateOffset(days=1)\n",
    "    next_day_str = next_day.strftime('%Y-%m-%d')\n",
    "    \n",
    "    next_day_price = get_stock_price(symbol, next_day_str)\n",
    "    if next_day_price is None:\n",
    "        return None\n",
    "    return 'up' if next_day_price > today_price else 'down'\n",
    "\n",
    "def calculate_accuracy(predictions, articles, symbol): # ACCURACY OF MODEL\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(predictions)\n",
    "\n",
    "    for idx, (prediction, article_date) in enumerate(zip(predictions, articles)):\n",
    "        price_change = get_price_change(symbol, article_date)\n",
    "        \n",
    "        if price_change is None:\n",
    "            continue\n",
    "\n",
    "        if (prediction == 'positive' and price_change == 'up') or (prediction == 'negative' and price_change == 'down'): # CALCULATE IF CORRECT/INCORRECT\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model with K-Fold Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment prediction accuracy for AAPL (cross-validation): 0.5020618556701031\n",
      "Average sentiment prediction accuracy for AMZN (cross-validation): 0.5390295004089498\n",
      "Average sentiment prediction accuracy for TSLA (cross-validation): 0.5462040816326531\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # GPU OPTIMIZATION\n",
    "model.to(device)\n",
    "\n",
    "def get_sentiment(titles, model, device): # SENTIMENT TAGGING\n",
    "    model.eval()\n",
    "    sentiments = []\n",
    "    \n",
    "    for title in titles:\n",
    "        inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device) #FORMAT DATA FOR TENSOR\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits).item()\n",
    "        \n",
    "        if prediction == 4:\n",
    "            sentiments.append('positive')\n",
    "        else:\n",
    "            sentiments.append('negative')\n",
    "    \n",
    "    return sentiments\n",
    "\n",
    "def cross_validate(titles, dates, model, device, symbol, n_splits=5): # CROSS VALIDATION FOR N_SPLITS\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # KFOLD CV\n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(titles):\n",
    "        train_titles, test_titles = [titles[i] for i in train_index], [titles[i] for i in test_index]\n",
    "        train_dates, test_dates = [dates[i] for i in train_index], [dates[i] for i in test_index]\n",
    "        \n",
    "        train_predictions = get_sentiment(train_titles, model, device)\n",
    "        test_predictions = get_sentiment(test_titles, model, device) \n",
    "        \n",
    "        accuracy = calculate_accuracy(test_predictions, test_dates, symbol) # CALCULATE ACCURACY FOR KFOLD CV\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies) # AVG ACCURACY FOR ALL FOLDS\n",
    "    return avg_accuracy\n",
    "\n",
    "symbols = [\"AAPL\", \"AMZN\", \"TSLA\"]\n",
    "datasets = [\n",
    "    (titles_apple, dates_apple, \"AAPL\"),\n",
    "    (titles_amazon, dates_amazon, \"AMZN\"),\n",
    "    (titles_tesla, dates_tesla, \"TSLA\")\n",
    "]\n",
    "\n",
    "for titles, dates, symbol in datasets:\n",
    "    avg_accuracy = cross_validate(titles, dates, model, device, symbol, n_splits=5) # 5 SPLITS\n",
    "    print(f\"Average sentiment prediction accuracy for {symbol} (cross-validation): {avg_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
